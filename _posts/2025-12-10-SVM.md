---
layout: post

title: User Classification using Support-Margin SVM

tags: [Calculus , CSE, Data Science, AI, Optimization]

feature-img: "assets/img/calculus/svm.png"

thumbnail: "assets/img/calculus/svm.png"

categories: CSE, AI, Calculus, Optimization, Gradient, Directional Derivative
---


## User Classification using Support-Margin SVM

빅데이터 수학 기말 과제 <br>

**Author : 고건우, 김민성, 손승현** <br>

<br>

## Introduction

유저 분류는 사용자 행동 데이터를 기반하여 분류하는 문제로, 서비스 개선과 의사결정 과정에서 중요한 역할을 한다. 이런 분류 문제에서는 분류할 두 클래스 간의 경계를 명확히 정의해서 일반화 성능을 이끌어 내는 것이 중요하다. <br>

본 프로젝트에서 사용하는 Support Vector Machinie (SVM)은 Margin 최대화를 이용한 초평면을 학습하여 분류하는 기계학습 방법으로 유저 분류를 수행하고 성능을 평가한다. <br>

## 1. Data Characteristics Analysis

분류를 하기 전에 데이터의 feature와 구성을 확인해보자

```python
import pandas as pd

df = pd.read_csv("data.csv")
print(df.head())
```

**output :**

<img width="821" height="212" alt="image" src="https://github.com/user-attachments/assets/04484c97-eac8-4bf5-906d-088609fe46f1" />

feature 3개와 label이 주어졌음을 알 수 있고, 이를 통해 3차원 상의 svm을 수행해야 하는 것을 알 수 있다. <br>



## 2. Support Vector Machine

**서포트 벡터 머신(Support Vector Machine, SVM)** 은 고차원 데이터에서도 강력한 분류 성능을 보이는 대표적인 지도학습 기법이다. <br>

Random forest, NeuralNet과 더불어 가장 많이 사용되는 ML기법으로 관련 전공자라면 한번쯤은 들어봤을법한 기법이다. <br>

SVM의 핵심 아이디어는 주어진 데이터들을 **가장 넓은 마진(margin)** 을 확보할 수 있도록 분리하는 초평면(hyperplane)을 찾는 것이다. <br>

<img width="480" height="480" alt="image" src="https://github.com/user-attachments/assets/b40dd590-be06-47b8-a61e-e9e1801006b5" />


이때 초평면과 가장 가까이 위치한 데이터 점들을 **서포트 벡터(support vectors)** 라 부르며, 이들이 결정 경계(decision boundary)를 정의하는 데 핵심적인 역할을 한다. <br>



### 2.1 Description of SVM

SVM의 핵심은 고차원에서 두개의 그룹을 가장 넓게 분리하는 가상의 초평면(hyperplane)을 찾아내는 것이다.

위 그림에서 각 그룹에서 가상의 구분선 까지의 거리가 가장 짧은 데이터를 **support vector** 라고 부른다.<br>



### 2.2 Mathematical Principles of SVM

SVM의 결과 목표가 다음과 같다고 가정해보자

<img width="480" height="480" alt="image" src="https://github.com/user-attachments/assets/91209d3c-ec4d-4e5c-8808-f41c23f232f4" />


지금은 2차원 데이터이기 때문에 hyperplane은 1차원 직선이고 다음과 같이 쓸 수 있다

$$
w^Tx + b = 0
$$

각 데이터의 범주와 결정경계와의 거리가 c 라고 가정할때 범주를 다음과 같이 쓸 수 있다. <br>

빨간데이터 :

$$
w_{1}x_{1} + w_{2}x_{2} + b \geq c
$$

파란데이터 :

$$
w_{1}x_{1} + w_{2}x_{2} + b \leq -c
$$

여기서 초평면은 어떠한 상수를 곱해도 같은 초평면이 나오기 때문에 다음과 같이 스케일링할 수 있다.<br>

**hyperplane scaling**

$$
w^{T}x + b = c
$$

$$
(cw)^Tx + cb = c
$$

$$
c(w^Tx + b) = c
$$

$$
w^Tx + b = 1
$$

또한 두 직선사이의 거리 공식 (평면 간의 거리 공식도 동일하다)을 이용하여 margin의 거리도 일반화 시킬 수 있다

**Remind: distance between two line** <br>

$$
\text{line1} = ax + by + c_{2} = 0
$$

$$
\text{line2} = ax + by + c_{1} = 0
$$

$$
distance : d = \frac{|c_{1} - c_{2}|}{\sqrt{w_1^2 + w_2^2}}
$$

위 공식을 경계조건을 통해 대입해 보면 다음과 같이 계산할 수 있다.

$$
d = \frac{|(b-1) - (b+1)|}{\sqrt{w_1^2 + w_2^2}} = \frac{2}{\sqrt{w_1^2 + w_2^2}}
$$

따라서 우리의 목표인 margin maximization을 위해서는 결국 분모를 최소화 시키면 되고, <br>

이는 곧 norm을 최소화 시키면 된다. <br>

SVM에서는 계산의 편의를 위해 다음을 최소화 하는 방향으로 식을 세운다.

$$
(1/2)\|w\|^2
$$

그리고 빨간데이터와 파란데이터를 동시에 만족시키는 조건은 다음과 같은 식 하나로 표현할 수 있다.

$$
y(w_{1}x_{1} + w_{2}x_{2} + b) \geq 1
$$

정리해 보면, 다음과 같은 문제로 정의할 수 있다. <br>
"다음과 같은 경계조건(constraints)에서의 margin의 최대를 구하여라" <br>
이는 곧 제약조건의 최대 최소 (최적화) 문제라고 볼 수 있다. <br>
<br>
SVM의 최적화는 이런 경계조건에서의 최적화문제에서 많이 쓰이는 방법인 Lagrange multiplier method를 사용해서 해결한다. <br>
<br>

**문제 정의** <br>

$$
min \ \frac{1}{2}\|w\|^2
$$

$$
s.t. \ y(w^T + b) \geq 1
$$

이를 통해 Lagrange Function을 다음과 같이 정의할 수 있다

$$
L(w, b, \alpha) = \frac{1}{2}\|w\|^2 - \sum_i \alpha_i \big[ y_i (w^T x_i + b) - 1 \big]
$$

그리고 이후에 이 알고리즘에 핵심인 Karush-Kuhn-Tucker (KKT) conditions을 사용한다. <br>

KKT 조건은 다음과 같다 <br>

① **Stationarity** <br>

② **Primal feasibility** <br>

③ **Dual feasibility** <br>

④ **Complementray slackness** <br>

<br>

**KKT 조건 1 : Stationarity (정지조건)** <br>

우리는 부등식이라는 constraints condition이 있기 때문에 경계에 걸려서 더 못내려가는 상태를 찾는 것이고, 최적점이 제약경계 내부에 있다면 상관없지만 제약조건 밖에 있을때는 더 못내려가 가게 막아줘야 한다. <br>
이제 gradient를 구해보자. gradient는 각각의 변수에 대해 편미분을 해주면 된다. <br>
그리고 더 내려가지못하게 막아줘야 하는 것은, gradient(경사)가 0임을 의미한다.

$$
\frac{\partial \mathcal{L}}{\partial w}
= w - \sum_i \alpha_i y_i x_i = 0
\Rightarrow w = \sum_i \alpha_i y_i x_i
$$

$$
\frac{\partial L}{\partial b} = \sum_i \alpha_i y_i = 0
$$

그리고 다음 KKT조건으로 넘어가기 전에 위를 통해 구한 식을 대입함으로써, 결정함수를 얻어낼 수 있다.<br> 

**Decision Function**

$$
w \cdot x + b 
= \left( \sum_i \alpha_i y_i x_i \right) \cdot x + b 
= \sum_i \alpha_i y_i (x_i \cdot x) + b
$$

이렇게 얻어낸 식에서 부호를 통해 입력 데이터를 분류해 낼 수 있다. <br>

$$
f(x) = \text{sign}\left( \sum_i \alpha_i y_i (x_i \cdot x) + b \right)
$$

부호를 통해 얻을 수 있는 이유는, 내적을 통해 간접적으로 코사인유사도를 쓰기 때문이다. <br>

**Cosine Similarity** <br>
1에 가까울수록 두 데이터의 유사도는 비슷하고 (cos 0 = 1) -1에 가까울수록 (cos 180 = -1) 연관도가 낮다


**KKT 조건 2 : Primal feasibility(타당성) , KKT 조건 4 : Complementary slackness (상보조건)** <br>

조건 2 는 제약조건이 넘어가는 점이 없어야 한다는 조건이고, 조건 4는 라그랑즈 상수가 >= 0이여야 한다는 조건이다. <br>

$$
\alpha_i \big[\, y_i (w^T x_i + b) - 1] = 0
$$

이 조건을 통해 서포트 벡터가 아닌 내부의 점은 다음과 같다

$$
 y_i (w^T x_i + b) - 1 > 0
$$

그러면 처음 식을 만족시키기 위해서는 α가 0을 만족해야 한다. <br> 

그리고 경계의 점 (서포트 벡터)일 경우에는 다음과 같이 되고

$$
 y_i (w^T x_i + b) - 1 = 0
$$

그러면 α는 0이 아니게 된다. <br>


**KKT 조건 3 : Primal feasibility(듀얼 타당성)** <br>

마지막 조건은 이전에서 구한 α는 양수라는 조건이다. <br>

Lagrange 상수는 penalty 같은 존재이기 때문에 양수여야 한다. 이 조건을 통해 자동으로 서포트벡터를 걸러낼 수 있다 <br>

결론적으로 라그랑즈 승수법에서 KKT조건을 이용한 최적화는 α가 최대이면서 w, b가 최소인 saddle point를 구하는 과정이다. <br>

그리고 w,b를 primal 변수(margin을 위한 최소화)라고 하고 α를 dual 변수(제약 위반을 막음)라고 한다. <br>

margin을 최대화시켜주는 margin의 경계에 있고, margin위에 있기때문에 α > 0인 지점 이는 곧, 다음 조건을 형성해서 풀어주면 된다.

$$
y_i (w^T x_i + b) - 1 = 0
$$
<br>


## 3.Soft-Margin SVM

데이터를 경계를 나눠서 분류할 수 있다면 좋겠지만, 현실세계 데이터는 정확하게 분류하기 힘들다. <br>
따라서 완벽하기 분리시키는 것이 아닌, 약간의 오류를 허용하는 것이 soft-margin SVM이다. <br>

<img width="599" height="386" alt="image" src="https://github.com/user-attachments/assets/08bd091b-19bf-4024-80c6-71a9dfdac218" />

그리고 그 약간의 오류를 슬랙변수 ξ를 넣어서 제약식을 세롭게 정의한다. <br>

$$
y_i (w^T x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
$$

여기서 ξ는 데이터가 경계를 얼마나 침범했는지를 나타내는 양의 실수 이다. <br>
① ξ = 0 : 완벽한 분류 <br>
② 0 < ξ < 1 : margin 침범 (약한 패널티) <br>
③ ξ > 1 : 오분류 (강한 패널티) <br>

그리고 이를 통해 최적화 문제를 다시 정의할 수 있다.

$$
\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{n}\xi_i
$$

여기서 C는 우리가 정의하는 얼마나 침범을 허용할 것인가에 대한 변수이다. <br>
결론적으로는 margin을 최대화하면서 벌점인 슬랙변수를 최소화하는 최적화 지점을 찾는 것이 목표가 된다. <br>
<br>
Hard-Margin 이랑 같은 방식으로 Lagrange를 KKT조건에 맞게 풀어보자. <br>
새로운 최적화 문제를 Lagrange로 정의하면 다음과 같이 쓸 수 있다.

$$
L(w,b,\xi,\alpha,\mu)
= \frac{1}{2}\|w\|^2 
+ C\sum_{i=1}^{n}\xi_i
- \sum_{i=1}^{n}\alpha_i [y_i(w^T x_i + b) - 1 + \xi_i]
- \sum_{i=1}^{n}\mu_i\xi_i
$$

하드마진과 달리 라그랑즈 상수가 한개 더 늘어난 것을 알 수 있다. <br>

정지조건에 맞게 편미분을 해주자. <br>

$$
\frac{\partial L}{\partial w} 
= w - \sum_i \alpha_i y_i x_i = 0 
\quad \Rightarrow \quad 
w = \sum_i \alpha_i y_i x_i
$$

$$
\frac{\partial L}{\partial b} 
= -\sum_i \alpha_i y_i = 0 
\quad \Rightarrow \quad 
\sum_i \alpha_i y_i = 0
$$

$$
\frac{\partial L}{\partial \xi_i} 
= C - \alpha_i - \mu_i = 0 
\quad \Rightarrow \quad 
\alpha_i \le C
$$

여기서 하드마진과 달리 슬랙변수에 대한 편미분에서 부등식이 나오는것을 알 수 있다<br>
이유는 μ또한 라그랑즈 상수이기 때문에 0이상이다. <br>
αi​ + μi​ = C 여기서 αi​ - C = −μi 이렇게 ​쓸 수 있고, -μi <= 0 이를 대입해서 <br>
αi​ - C <= 0 따라서 ai <= C가 된다. <br>
<br>
결국 슬랙변수는 미분으로 제거가 되고, α와 C 사이의 관계식으로 표현이 된다. <br>
그리고 대입을 할때 w 제곱에 α에 대한 식이 들어감으로써 2차원 convex optimization이 된다

$$
\max_{\alpha} 
\sum_{i=1}^{n} \alpha_i 
- \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j)
$$

$$
subject\ to \quad
0 \le \alpha_i \le C, 
\quad
\sum_{i=1}^{n} \alpha_i y_i = 0
$$

우리는 C를 조절하면서 어느정도까지 오분류를 허용할지 정할 수 있다. <br>
이를 기반으로 코드로 구현해 유저 분류를 해보자

## 4. User Classification using Soft-Margin SVM

### 4.1 import Library

```python
import numpy as np
```
<br>


### 4.2 Class SoftMargin SVM

```python
class SoftMarginSVM:
    def __init__(self, C=10.0, tol=1e-3, conv=10):
        self.C = C
        self.tol = tol
        self.conv = conv
        self.rng = np.random.default_rng()

    # 4.3 후술
    def fit(self):
        pass
    
    # 4.4 후술
    def predict(self):
        pass
```

C가 위에서 정한 얼마나 오분류를 허용하는지에 대한 hyperparameter이다 <br>
tol은 KKT 조건에서 구한 sum α = 1 이 정확하게 만족하지 않아 0.99999일 경우 만족하지 않는다고 판단하여 계속 계산이 수행될 수 있기 때문에 어느정도 만족하면 멈춰준다. <br>
conv는 10번 update에서 만족하면 수렴으로 간주하고 중지한다<br>



### 4.3 Fitting function

Soft-Maring SVM의 듀얼함수는 다음과 같이 나왔다

$$
\max_{\alpha}
\sum_{i=1}^{n} \alpha_i
- \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j)
$$

$$
subject\ to \quad
0 \le \alpha_i \le C,
\quad
\sum_{i=1}^{n} \alpha_i y_i = 0

$$

함수에서 우리는 αᵢ와 α_j 두개의 변수를 최적화 시켜야 한다. <br>

하지만 KKT의 다음 조건으로 인해 αᵢ를 αⱼ에 의해 자동으로 결정되게 만듦으로 빠른 계산을 수행하게 할 수 있다<br>

**SMO optimization**

$$
\sum_k\alpha_ky_k = 0
$$

$$
\alpha_iy_i + \alpha_jy_j = Constant
$$

여기서 어떠한 상수는 업데이트 전의 기존 값(정해진 상수)으로 결정된다

$$
Constant = \alpha_i^{old}y_i + \alpha_j^{old}y_j
$$

이 과정으로 αᵢ를 αⱼ로 표현하여 αⱼ만 업데이트하면서 αᵢ를 동시에 업데이트를 할 수 있다

$$
\alpha_iy_i + \alpha_jy_j = \alpha_i^{old}y_i + \alpha_j^{old}y_j
$$

$$
\alpha_iy_i  = \alpha_i^{old}y_i + \alpha_j^{old}y_j- \alpha_jy_j
$$

$$
\alpah_i = \alpha_i^{old} + y_iy_j(\alpha_j^{old} - \alpha_j)
$$

그리고 αⱼ는 다음과 같은 step으로 계산한다

$$
\alpha_j^{new} = \alpha_j^{old} + \frac{y_j(E_i - E_j)}{\eta}
$$

E는 예측오차인데 실제값 - 예측값이다. 이 값에 따라 αⱼ의 방향성을 얻을 수 있다 <br>

η는 곡률로 클수록 step사이즈가 작아지고 작으면 step이 커진다.

```python
def fit(self, X, y01):

    X = np.asarray(X, float)

    # SVM은 -1과 1로 수행하지만 주어진 데이터는 0,1로 구분되어있기때문에 -1,1로 변환
    y = np.where(y01 == 0, -1.0, 1.0)

    n, d = X.shape

    # Lagrange multiplier
    self.alpha = np.zeros(n)

    # bias (wx + b = 0 에서 b)
    self.b = 0.0

    # Inner product
    K = X @ X.T

    passes = 0

    # α가 수렴시 종료 (conv 횟수만큼 동일할 시)
    while passes < self.conv:

        num_changed = 0

        # 모든 데이터에 대해 수행
        for i in range(n):

            # Decision fuction
            f_i = np.sum(self.alpha * y * K[:, i]) + self.b

            # 예측오차 (실제값 - 예측값)
            E_i = f_i - y[i]

            # Soft Margin KKT 조건 식 만족 확인
            # 첫번째 조건은 마진 안쪽에 존재하지만 αⱼ가 C보가 작기때문에 업데이트 필요
            # 두번째 조건은 마진 바깥쪽이지만 αⱼ가 0이 아니기때문에 업데이트가 필요하다
            if ((y[i]*E_i < -self.tol and self.alpha[i] < self.C) or
                (y[i]*E_i >  self.tol and self.alpha[i] > 0)):

                # 동시에 αᵢ, αⱼ를 최적화하고있기때문에 그에 상응하는 label 필요 (데이터 하나 더 뽑음)
                # 두 데이터가 같으면 의미가 없기때문에 같다고 전제로 해놓고 같을경우 다른 데이터 배정으로 뽑는다
                j = i
                while j == i:
                    j = self.rng.integers(0, n)

                # dual function 예측값
                f_j = np.sum(self.alpha * y * K[:, j]) + self.b

                # 오차 재계산
                E_j = f_j - y[j]

                # 변수 업데이트
                ai_old, aj_old = self.alpha[i], self.alpha[j]

                # αⱼ를 box constraints에 맞춰 KKT의 0 ≤ αⱼ ≤ C에 맞춰준다

                # 두 데이터가 서로 다른 클래스일 경우
                if y[i] != y[j]:

                    # 작을시 0
                    # 등식 제약을 만족시키기 위해 빼면서 조정 필요
                    L = max(0, aj_old - ai_old)

                    # C보다 클경우 C
                    H = min(self.C, self.C + aj_old - ai_old)

                # 같은 클래스 일 경우
                else:
                    # 작을시 0
                    # 등식 제약을 만족시키기 위해 더하면서 조정 필요
                    L = max(0, ai_old + aj_old - self.C)

                    # C보다 클경우 C
                    H = min(self.C, ai_old + aj_old)

                # 최적화 필요 없음
                if L == H:
                    continue
                
                # step size η (curvature, 2차 미분)
                eta = 2*K[i,j] - K[i,i] - K[j,j]
                
                # 곡률이 0일 경우 convex이기 때문에 움직이면 최소화방향으로 움직이기 때문에 업데이트를 하지 않는다
                if eta >= 0:
                    continue

                # 구한 step size와 기울기에 맞게 업데이트
                self.alpha[j] -= y[j]*(E_i - E_j)/eta

                # Box contraints 만족
                self.alpha[j] = np.clip(self.alpha[j], L, H)

                # 업데이트 변화가 미미할시 다음 단계로 넘어간다
                if abs(self.alpha[j] - aj_old) < 1e-10:
                    continue
                
                # 업데이트 시킨 αⱼ에 맞춰 αᵢ도 업데이트 시킨다
                self.alpha[i] += y[i]*y[j]*(aj_old - self.alpha[j])

                # 구한 두 변수에 대해 b를 KKT 조건식에 맞춰 재계산
                b1 = (self.b - E_i
                        - y[i]*(self.alpha[i]-ai_old)*K[i,i]
                        - y[j]*(self.alpha[j]-aj_old)*K[i,j])

                b2 = (self.b - E_j
                        - y[i]*(self.alpha[i]-ai_old)*K[i,j]
                        - y[j]*(self.alpha[j]-aj_old)*K[j,j])

                # 둘중 서포트 벡터 만족시 b 업데이트
                if 0 < self.alpha[i] < self.C:
                    self.b = b1
                elif 0 < self.alpha[j] < self.C:
                    self.b = b2

                # 둘다 서포트벡터가 아닐시 평균으로 상정
                else:
                    self.b = 0.5*(b1 + b2)

                num_changed += 1

        # 변화가 없을시 수렴 횟수를 위해 카운트를 올리고 아닐시 업데이트
        passes = passes + 1 if num_changed == 0 else 0

    # w 재계산
    self.w = np.sum((self.alpha * y)[:, None] * X, axis=0)

    # 구한 hyperplane return
    return self
```
<br>


### 4.4 Predict Function

학습된 hyperplane에 대한 결정함수를 사용해 분류를 하는 함수이다 <br>
양수면 1클래스, 음수면 -1클래스 0일경우에는 hyperplane 위이다 <br>
원래는 -1,1로 나와야 하지만 데이터가 0,1 label로 주어졌기때문에 0,1로 맞춰주었다

```python
def predict(self, X):
    scores = X @ self.w + self.b
    return np.where(scores >= 0, 1, 0)
```
<br>


### 전체 코드

```python
class SoftMarginSVM:
    def __init__(self, C=10.0, tol=1e-3, conv=10):
        self.C = C
        self.tol = tol
        self.conv = conv
        self.rng = np.random.default_rng()

    def fit(self, X, y01):
        X = np.asarray(X, float)
        y = np.where(y01 == 0, -1.0, 1.0)

        n, d = X.shape
        self.alpha = np.zeros(n)
        self.b = 0.0

        K = X @ X.T

        passes = 0
        while passes < self.conv:
            num_changed = 0
            for i in range(n):
                f_i = np.sum(self.alpha * y * K[:, i]) + self.b
                E_i = f_i - y[i]

                if ((y[i]*E_i < -self.tol and self.alpha[i] < self.C) or
                    (y[i]*E_i >  self.tol and self.alpha[i] > 0)):

                    j = i
                    while j == i:
                        j = self.rng.integers(0, n)

                    f_j = np.sum(self.alpha * y * K[:, j]) + self.b
                    E_j = f_j - y[j]

                    ai_old, aj_old = self.alpha[i], self.alpha[j]

                    if y[i] != y[j]:
                        L = max(0, aj_old - ai_old)
                        H = min(self.C, self.C + aj_old - ai_old)
                    else:
                        L = max(0, ai_old + aj_old - self.C)
                        H = min(self.C, ai_old + aj_old)

                    if L == H:
                        continue

                    eta = 2*K[i,j] - K[i,i] - K[j,j]
                    if eta >= 0:
                        continue

                    self.alpha[j] -= y[j]*(E_i - E_j)/eta
                    self.alpha[j] = np.clip(self.alpha[j], L, H)

                    if abs(self.alpha[j] - aj_old) < 1e-10:
                        continue

                    self.alpha[i] += y[i]*y[j]*(aj_old - self.alpha[j])

                    b1 = (self.b - E_i
                          - y[i]*(self.alpha[i]-ai_old)*K[i,i]
                          - y[j]*(self.alpha[j]-aj_old)*K[i,j])
                    b2 = (self.b - E_j
                          - y[i]*(self.alpha[i]-ai_old)*K[i,j]
                          - y[j]*(self.alpha[j]-aj_old)*K[j,j])

                    if 0 < self.alpha[i] < self.C:
                        self.b = b1
                    elif 0 < self.alpha[j] < self.C:
                        self.b = b2
                    else:
                        self.b = 0.5*(b1 + b2)

                    num_changed += 1

            passes = passes + 1 if num_changed == 0 else 0

        self.w = np.sum((self.alpha * y)[:, None] * X, axis=0)
        return self

    def predict(self, X):
        scores = X @ self.w + self.b
        return np.where(scores >= 0, 1, 0)

```
<br>


### 4.5 split data

주어진 데이터가 81개 밖에 없지만 학습데이터와 테스트 데이터가 따로 주어지지 않았기 때문에, 8:2로 나눠서 학습과 테스트를 진행한다

```python
def split_data(X, y, ratio=0.2):
    rng = np.random.default_rng()
    idx0 = np.where(y == 0)[0]
    idx1 = np.where(y == 1)[0]

    rng.shuffle(idx0)
    rng.shuffle(idx1)

    n0_test = int(len(idx0) * ratio)
    n1_test = int(len(idx1) * ratio)

    test_idx = np.concatenate([idx0[:n0_test], idx1[:n1_test]])
    train_idx = np.concatenate([idx0[n0_test:], idx1[n1_test:]])

    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]
```
<br>


### 4.6 Evaluation metrics

대표적인 평가지표인 Precision, Recall, F1-score를 사용해 모델을 평가한다 <br>
F1-score는 Precision과 Recall의 조화 평균으로, 클래스 불균형 상황에서 모델의 성능을 종합적으로 평가하는 데 적합하다.

```python
def f1_score(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))

    precision = tp / (tp + fp + 1e-12)
    recall    = tp / (tp + fn + 1e-12)
    f1 = 2 * precision * recall / (precision + recall + 1e-12)

    return precision, recall, f1
```
<br>


### 4.7 main code

```python
if __name__ == "__main__":

    data = np.loadtxt("data.csv", delimiter=",", skiprows=1)
    X = data[:, :-1]
    y = data[:, -1].astype(int)

    # normalization
    X = (X - X.mean(axis=0)) / X.std(axis=0)

    X_train, X_test, y_train, y_test = split_data(X, y)
    
    # C = 10
    svm = SoftMarginSVM(C=10.0)
    svm.fit(X_train, y_train)

    y_pred = svm.predict(X_test)

    p, r, f1 = f1_score(y_test, y_pred)

    print("Precision:", p)
    print("Recall   :", r)
    print("F1-score :", f1)
```
<br>



### 4.8 output

데이터 스플릿에 따라 0.8333 ~ 0.999 사이의 f1-score값이 나왔다

<img width="492" height="122" alt="image" src="https://github.com/user-attachments/assets/ebb171ac-0a01-475a-b80e-6de5cd9d595a" />

