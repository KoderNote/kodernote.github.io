---
layout: post

title: User Classification using Support-Margin SVM

tags: [Calculus , CSE, Data Science, AI, Optimization]

feature-img: "assets/img/calculus/gradient.png"

thumbnail: "assets/img/calculus/gradient.png"

categories: CSE, AI, Calculus, Optimization, Gradient, Directional Derivative
---


## User Classification using Support-Margin SVM

빅데이터 수학 기말 과제 <br>

**Author : 고건우, 김민성, 손승현** <br>

<br>

## Introduction

유저 분류는 사용자 행동 데이터를 기반하여 분류하는 문제로, 서비스 개선과 의사결정 과정에서 중요한 역할을 한다. 이런 분류 문제에서는 분류할 두 클래스 간의 경계를 명확히 정의해서 일반화 성능을 이끌어 내는 것이 중요하다. <br>

본 프로젝트에서 사용하는 Support Vector Machinie (SVM)은 Margin 최대화를 이용한 초평면을 학습하여 분류하는 기계학습 방법으로 유저 분류를 수행하고 성능을 평가한다. <br>

## 1. Data Characteristics Analysis

분류를 하기 전에 데이터의 feature와 구성을 확인해보자

```python
import pandas as pd

df = pd.read_csv("data.csv")
print(df.head())
```

**output :**

feature 3개와 label이 주어졌음을 알 수 있고, 이를 통해 3차원 상의 svm을 수행해야 하는 것을 알 수 있다.

## 2. Support Vector Machine

**서포트 벡터 머신(Support Vector Machine, SVM)** 은 고차원 데이터에서도 강력한 분류 성능을 보이는 대표적인 지도학습 기법이다. <br>

Random forest, NeuralNet과 더불어 가장 많이 사용되는 ML기법으로 관련 전공자라면 한번쯤은 들어봤을법한 기법이다. <br>

SVM의 핵심 아이디어는 주어진 데이터들을 **가장 넓은 마진(margin)** 을 확보할 수 있도록 분리하는 초평면(hyperplane)을 찾는 것이다. <br>

이때 초평면과 가장 가까이 위치한 데이터 점들을 **서포트 벡터(support vectors)** 라 부르며, 이들이 결정 경계(decision boundary)를 정의하는 데 핵심적인 역할을 한다. <br>

### 2.1 Description of SVM

SVM의 핵심은 고차원에서 두개의 그룹을 가장 넓게 분리하는 가상의 초평면(hyperplane)을 찾아내는 것이다.

위 그림에서 각 그룹에서 가상의 구분선 까지의 거리가 가장 짧은 데이터를 **support vector** 라고 부른다.<br>

### 2.2 Mathematical Principles of SVM

SVM의 결과 목표가 다음과 같다고 가정해보자

지금은 2차원 데이터이기 때문에 hyperplane은 1차원 직선이고 다음과 같이 쓸 수 있다

$$
w^Tx + b = 0
$$

각 데이터의 범주와 결정경계와의 거리가 c 라고 가정할때 범주를 다음과 같이 쓸 수 있다. <br>

빨간데이터 :

$$
w_{1}x_{1} + w_{2}x_{2} + b \geq c
$$

파란데이터 :

$$
w_{1}x_{1} + w_{2}x_{2} + b \leq -c
$$

여기서 초평면은 어떠한 상수를 곱해도 같은 초평면이 나오기 때문에 다음과 같이 스케일링할 수 있다.<br>

**hyperplane scaling**

$$
w^{T}x + b = c
$$

$$
(cw)^Tx + cb = c
$$

$$
c(w^Tx + b) = c
$$

$$
w^Tx + b = 1
$$

또한 두 직선사이의 거리 공식 (평면 간의 거리 공식도 동일하다)을 이용하여 margin의 거리도 일반화 시킬 수 있다

**Remind: distance between two line** <br>

$$
\text{line1} = ax + by + c_{2} = 0
$$

$$
\text{line2} = ax + by + c_{1} = 0
$$

$$
distance : d = \frac{|c_{1} - c_{2}|}{\sqrt{w_1^2 + w_2^2}}
$$

위 공식을 경계조건을 통해 대입해 보면 다음과 같이 계산할 수 있다.

$$
d = \frac{|(b-1) - (b+1)|}{\sqrt{w_1^2 + w_2^2}} = \frac{2}{\sqrt{w_1^2 + w_2^2}}
$$

따라서 우리의 목표인 margin maximization을 위해서는 결국 분모를 최소화 시키면 되고, <br>

이는 곧 norm을 최소화 시키면 된다. <br>

SVM에서는 계산의 편의를 위해 다음을 최소화 하는 방향으로 식을 세운다.

$$
(1/2)\|w\|^2
$$

그리고 빨간데이터와 파란데이터를 동시에 만족시키는 조건은 다음과 같은 식 하나로 표현할 수 있다.

$$
y(w_{1}x_{1} + w_{2}x_{2} + b) \geq 1
$$

정리해 보면, 다음과 같은 문제로 정의할 수 있다. <br>
"다음과 같은 경계조건(constraints)에서의 margin의 최대를 구하여라" <br>
이는 곧 제약조건의 최대 최소 (최적화) 문제라고 볼 수 있다. <br>
<br>
SVM의 최적화는 이런 경계조건에서의 최적화문제에서 많이 쓰이는 방법인 Lagrange multiplier method를 사용해서 해결한다. <br>
<br>

**문제 정의** <br>
$$
min \ \frac{1}{2}\|w\|^2
$$

$$
s.t. \ y(w^T + b) \geq 1
$$

이를 통해 Lagrange Function을 다음과 같이 정의할 수 있다

$$
L(w, b, \alpha) = \frac{1}{2}\|w\|^2 - \sum_i \alpha_i \big[ y_i (w^T x_i + b) - 1 \big]
$$

그리고 이후에 이 알고리즘에 핵심인 Karush-Kuhn-Tucker (KKT) conditions을 사용한다. <br>

KKT 조건은 다음과 같다 <br>

① **Stationarity** <br>

② **Primal feasibility** <br>

③ **Dual feasibility** <br>

④ **Complementray slackness** <br>

<br>

**KKT 조건 1 : Stationarity (정지조건)** <br>

우리는 부등식이라는 constraints condition이 있기 때문에 경계에 걸려서 더 못내려가는 상태를 찾는 것이고, 최적점이 제약경계 내부에 있다면 상관없지만 제약조건 밖에 있을때는 더 못내려가 가게 막아줘야 한다. <br>
이제 gradient를 구해보자. gradient는 각각의 변수에 대해 편미분을 해주면 된다. <br>
그리고 더 내려가지못하게 막아줘야 하는 것은, gradient(경사)가 0임을 의미한다.

$$
\frac{\partial \mathcal{L}}{\partial w}
= w - \sum_i \alpha_i y_i x_i = 0
\Rightarrow w = \sum_i \alpha_i y_i x_i
$$

$$
\frac{\partial L}{\partial b} = \sum_i \alpha_i y_i = 0
$$

그리고 다음 KKT조건으로 넘어가기 전에 위를 통해 구한 식을 대입함으로써, 결정함수를 얻어낼 수 있다.<br> 

**Decision Function**

$$
w \cdot x + b 
= \left( \sum_i \alpha_i y_i x_i \right) \cdot x + b 
= \sum_i \alpha_i y_i (x_i \cdot x) + b
$$

이렇게 얻어낸 식에서 부호를 통해 입력 데이터를 분류해 낼 수 있다. <br>

$$
f(x) = \text{sign}\left( \sum_i \alpha_i y_i (x_i \cdot x) + b \right)
$$

부호를 통해 얻을 수 있는 이유는, 내적을 통해 간접적으로 코사인유사도를 쓰기 때문이다. <br>

**Cosine Similarity** <br>
1에 가까울수록 두 데이터의 유사도는 비슷하고 (cos 0 = 1) -1에 가까울수록 (cos 180 = -1) 연관도가 낮다

**KKT 조건 2 : Primal feasibility(타당성) , KKT 조건 4 : Complementary slackness (상보조건)** <br>

조건 2 는 제약조건이 넘어가는 점이 없어야 한다는 조건이고, 조건 4는 라그랑즈 상수가 >= 0이여야 한다는 조건이다. <br>

$$
\alpha_i \big[\, y_i (w^T x_i + b) - 1] = 0
$$

이 조건을 통해 서포트 벡터가 아닌 내부의 점은 다음과 같다

$$
 y_i (w^T x_i + b) - 1 > 0
$$

그러면 처음 식을 만족시키기 위해서는 α가 0을 만족해야 한다. <br> 

그리고 경계의 점 (서포트 벡터)일 경우에는 다음과 같이 되고

$$
 y_i (w^T x_i + b) - 1 = 0
$$

그러면 α는 0이 아니게 된다. <br>

**KKT 조건 3 : Primal feasibility(듀얼 타당성)** <br>

마지막 조건은 이전에서 구한 α는 양수라는 조건이다. <br>

Lagrange 상수는 penalty 같은 존재이기 때문에 양수여야 한다. 이 조건을 통해 자동으로 서포트벡터를 걸러낼 수 있다 <br>

결론적으로 라그랑즈 승수법에서 KKT조건을 이용한 최적화는 α가 최대이면서 w, b가 최소인 saddle point를 구하는 과정이다. <br>

그리고 w,b를 primal 변수(margin을 위한 최소화)라고 하고 α를 dual 변수(제약 위반을 막음)라고 한다. <br>

margin을 최대화시켜주는 margin의 경계에 있고, margin위에 있기때문에 α > 0인 지점 이는 곧, 다음 조건을 형성해서 풀어주면 된다.

$$
y_i (w^T x_i + b) - 1 = 0
$$



## Soft-Margin SVM

데이터를 경계를 나눠서 분류할 수 있다면 좋겠지만, 현실세계 데이터는 정확하게 분류하기 힘들다. <br>
따라서 완벽하기 분리시키는 것이 아닌, 약간의 오류를 허용하는 것이 soft-margin SVM이다. <br>
그리고 그 약간의 오류를 슬랙변수 ξ를 넣어서 제약식을 세롭게 정의한다. <br>

$$
y_i (w^T x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
$$

여기서 ξ는 데이터가 경계를 얼마나 침범했는지를 나타내는 양의 실수 이다. <br>
① ξ = 0 : 완벽한 분류 <br>
② 0 < ξ < 1 : margin 침범 (약한 패널티) <br>
③ ξ > 1 : 오분류 (강한 패널티) <br>

그리고 이를 통해 최적화 문제를 다시 정의할 수 있다.

$$
\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{n}\xi_i
$$

여기서 C는 우리가 정의하는 얼마나 침범을 허용할 것인가에 대한 변수이다. <br>
결론적으로는 margin을 최대화하면서 벌점인 슬랙변수를 최소화하는 최적화 지점을 찾는 것이 목표가 된다. <br>
<br>
Hard-Margin 이랑 같은 방식으로 Lagrange를 KKT조건에 맞게 풀어보자. <br>
새로운 최적화 문제를 Lagrange로 정의하면 다음과 같이 쓸 수 있다.

$$
L(w,b,\xi,\alpha,\mu)
= \frac{1}{2}\|w\|^2 
+ C\sum_{i=1}^{n}\xi_i
- \sum_{i=1}^{n}\alpha_i [y_i(w^T x_i + b) - 1 + \xi_i]
- \sum_{i=1}^{n}\mu_i\xi_i
$$

하드마진과 달리 라그랑즈 상수가 한개 더 늘어난 것을 알 수 있다. <br>

정지조건에 맞게 편미분을 해주자. <br>

$$
\frac{\partial L}{\partial w} 
= w - \sum_i \alpha_i y_i x_i = 0 
\quad \Rightarrow \quad 
w = \sum_i \alpha_i y_i x_i
$$

$$
\frac{\partial L}{\partial b} 
= -\sum_i \alpha_i y_i = 0 
\quad \Rightarrow \quad 
\sum_i \alpha_i y_i = 0
$$

$$
\frac{\partial L}{\partial \xi_i} 
= C - \alpha_i - \mu_i = 0 
\quad \Rightarrow \quad 
\alpha_i \le C
$$

여기서 하드마진과 달리 슬랙변수에 대한 편미분에서 부등식이 나오는것을 알 수 있다<br>
이유는 μ또한 라그랑즈 상수이기 때문에 0이상이다. <br>
αi​ + μi​ = C 여기서 αi​ - C = −μi 이렇게 ​쓸 수 있고, -μi <= 0 이를 대입해서 <br>
αi​ - C <= 0 따라서 ai <= C가 된다. <br>
<br>
결국 슬랙변수는 미분으로 제거가 되고, α와 C 사이의 관계식으로 표현이 된다. <br>
따라서 최종 식은 다음과 같다. 

$$
\max_{\alpha} 
\sum_{i=1}^{n} \alpha_i 
- \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j)
$$

$$
subject\ to \quad
0 \le \alpha_i \le C, 
\quad
\sum_{i=1}^{n} \alpha_i y_i = 0
$$

우리는 C를 조절하면서 어느정도까지 오분류를 허용할지 정할 수 있다. <br>
이를 기반으로 코드로 구현해 유저 분류를 해보자
