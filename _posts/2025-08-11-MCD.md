---
layout: post

title: Thesis review, Minimum Covariance Determinant

tags: [Linear Algebra, CSE, Data Science, AI, Statistics]

feature-img: "assets/img/0.post/linear/strang.jpg"

thumbnail: "assets/img/0.post/linear/strang.jpg"

categories: CSE, AI
---

## THESIS REVIEW: Minimum Covariance Determinant

이 글은 [**Hubert, M., & Debruyne, M. (2010). Minimum covariance determinant.**](https://wis.kuleuven.be/stat/robust/papers/2010/wire-mcd.pdf)

논문을 토대로 작성되었습니다. <br>

### 1. ABSTRACT

**MCD(Minimum Covariance Determinant)방법**은 빠른 알고리즘을 통해 **다변량 위치(location)** 와 **산포(scatter)** 의 **강건한(robust) 추정치**를 제공하는 기법이다. <br>

**공분산 행렬(Covariance matrix)** 는 다양한 통계방법의 초석이 되므로, MCD는 강건하고 계산적으로 효율적인 다변량 기법들을 개발하는데도 사용되고 있다. <br>

더 나아가 MCD는 **이상치(outlier) 감지를 위한 효율적이고 실용적인 도구**로 활용될 수 있어, 데이터 분석의 신뢰성을 높이는 데 기여한다. <br>

<br>

**MCD estimator**는 **affine equivariance**, **breakdown value**, 그리고 **influence function**의 주요 속성에 대한 내용과 <br>

affine equivariant이면서 robust를 상속하는 **빠른 결정론적 알고리즘**과 차원의 수가 case보다 많은 수 있는 고차원에 대해 설계되고 특이 행렬을 방지하기 위한 **정규화**에 대한 최근 확장 개념이 묘사되어있다. <br>

MCD는 의학, 금융, cv등 다양한 분야에 적용되고, **PCA(주성분분석)**, **Regression(회귀분석)**, **Factor analysis(요인분석)** 등 다변량 기술을 개발하는데도 사용되고 있다.

우선 MCD를 사용해야 하는 이유에 대한 예시를 살펴보자.

### 2. DESCRIPTION OF THE MCD ESTIMATOR

다변량 위치와 산포의 세팅에서, 데이터는 nxp 데이터 행렬에 저장되어있다고 가정한다.

$$
X=(x1​,…,xn​)⊤
$$

여기서 각 xi는 i번째 **관측값(i-th observation**이다.<br>

따라서 **n은 객체(object)의 개수**, **p는 변수(variable)의 개수**를 의미한다.

<br>

**Figure 1, Bivariate wine data with classical and robust tolerance ellipse**

사진 1위치

<br>

예시를 위해 **이변량(bivariate, p=2) 데이터 집합**을 고려한다.<br>

이 논문에서는 와인 데이터셋을 고려하는데 이 데이터셋은 3종류의 이탈리아 와인에서 발견된 13가지 성분의 양을 포함되어 잇고, 첫번째 그룹에 속한 59개의 와인중 **사과산(malic acid)**, **프롤린(proline)** 성분에 집중하고 있다. <br>

위 **Figure 1**에 그 데이터의 산점도(scatter plot)이 제시되어있고, **classical** 및 **rubust** 97.5% **허용타원(tolerance ellipse)** 가 함께 그려져있다.

<br>

#### 2.1 Mahalanobis distance (마할라노비스 거리)

여기서 잠시 mahalanobis distance의 개념에 대해 짚고 넘어가자. <br>

공식은 다음과 같다.

$$
MD(x) = \sqrt{(x - \bar{x})^{\top} S^{-1} (x - \bar{x})}
$$

우선 공식을 이해하기 위해서는 **이차형식(quadratic form)** 에 대한 개념을 알고 있어야 한다. <br> 공식 유도과정은 다음과 같다.

$$
\textbf{가정: }\;\Sigma \in \mathbb{R}^{p \times p}\text{ 는 대칭 양의정부호(SPD) 공분산 행렬, }\;\mu \in \mathbb{R}^p
$$

$$
\Sigma = U \Lambda U^\top,\quad
\Lambda = \mathrm{diag}(\lambda_1,\dots,\lambda_p),\;\lambda_i>0
$$

$$
\Sigma^{-1/2} = U \Lambda^{-1/2} U^\top,\quad
\Lambda^{-1/2}=\mathrm{diag}(\lambda_1^{-1/2},\dots,\lambda_p^{-1/2})
$$

$$
z := \Sigma^{-1/2}(x-\mu)
$$

$$
\|z\|_2^2
= (x-\mu)^\top \big(\Sigma^{-1/2}\big)^\top \Sigma^{-1/2} (x-\mu)
$$

$$
= (x-\mu)^\top \Sigma^{-1} (x-\mu)
$$

$$
\therefore\; MD(x) = \sqrt{(x-\mu)^\top \Sigma^{-1} (x-\mu)}
$$

쉽게 설명하자면 거리는 절대값이기때문에 루트에 제곱을 씌워 다음과같은 형태임을 알수있다.

$$
\sqrt{(x - \mu)^2}
$$

이는 내적의 정의에 따라 다음과 같다

$$
\sqrt{(x-\mu)^\top (x-\mu)}
$$

여기서 루트안에 잇는 다음의 식을 **이차형식**으로 생각하면 반지름이 1인 원이 나온다<br>

하지만 거리를 계산할때 **분산(variance)** 의 존재때문에 (퍼짐의정도) 거리가 객관적으로 나오지 않고 밀도가 높은 곳은 거리가 덜반영되고, 낮은 곳은 더 크게 반영되는 문제가 있다. <br>

따라서 공분산으로 나눠 밀도를 맞춰주고 이를 **whitening**이라고 한다. <br>

그리고 이 과정을 통해 **고유치(eigen value)** 를 계수로 삼는 **타원(ellipse)** 이 나온다. <br>

이 과정을 생각하면 다음의 공식으로 바로 유도할 수 있다

$$
\therefore\; MD(x) = \sqrt{(x-\mu)^\top \Sigma^{-1} (x-\mu)}
$$

<br>

이 마할라노비스거리는 점 x가 data cloud의 중심으로 부터, 그 크기에 비해 **얼마나 멀리 떨어져 있는지**를 알려준다.

**참고** <br>

확률분포에서 어느 방향으로의 퍼진정도가 **분산(variance)** 이고 이 분산은 다시 말해 **고유치(eigen value)** 가 된다.<br>

그리고 그 방향은 **고유벡터(eigen vector)** 가 된다.

<br>

#### 2.2 Robust Distance

그럼 본론으로 돌아와서 **classical**과 **robust** 의 **tolerance ellipse**를 비교해보자. <br>

**Classical tolerance ellipse**는 MD(x)가 **카이제곱분포**의 분위수가 같아지는 p-차원 점 x들의 집합으로 정의한다.

밑 **Figure 2**는 **Mahalanobis distance**와 **robust distance**를 비교한 그림이다.

<br>

**Figure 2**

그림2

<br>

Figure 2에서 보이다 싶이 기본 마할라노비스 거리의 타원은 모든 관측치를 포괄하려는 것을 알 수 있다. 따라서 이상치가 3개밖에 안보이는 것을 알 수 있다.<br>

반면에 Figure1의 **robust tolerance ellipse**는 다음식을 따르고 더 작고 정규화된 데이터 부분만 포함하고 있다.

$$
RD(x) = \sqrt{(x - \hat{\mu}_{\text{MCD}})^\top \hat{\Sigma}_{\text{MCD}}^{-1} (x - \hat{\mu}_{\text{MCD}})}
$$

여기서 μ_MCD는 위치의 mcd추정량이고, Σ_MCD는 mcd의 공분산 추정치 이다. <br>

이롤 이용하면 Figure 2(b)처럼 8개의 이상치와 1개의 이상치를 찾아내는 것을 볼 수 있다. <br>

**classical estimates**가 **이상치(outlying values)** 의 영향을 너무 크게 받아, MD(x)와 같은 확인 수단들이 더이상 이상치를 탐지할 수 없게 되는 현상을 **masking effect**라고 한다. <br>

이런 데이터의 신뢰할 수 있는 분석을 위해 **robust estimator**가 필요하다.

<br>

### 3. Definition

우선 [.]은 **바닥함수(floor function)** 을 나타낸다. <br>

x를 넘지않는 최대의 정수를 뜻하는 것으로 흔히 아는 **Gauss symbol**이다. <br>

매개변수 **[(n + p + 1)/2] ≤ h ≤ n** 갖는 **raw MCD 추정량**은 다음과 같은 **위치(location)** 와 **산포(dispersion)** 추정치를 정의한다. <br>

> μ_0은 표본 공분산 행렬의 행렬식이 최소가 되는 h개의 관측치들의 평균이다 <br>
> 
> Σ는 공분산행렬에 consistency factor(보정계수) **c**를 곱한 것이다.

<br>

여기서 MCD 추정량은 **h>p**일때만 계산이 가능한데, 그렇지 않으면 어떤 h의 subset의 공분산행렬이 **특이(singular)** 하게 된다. <br>

이유에 대해 잠시 알아보고 넘어가자

<br>