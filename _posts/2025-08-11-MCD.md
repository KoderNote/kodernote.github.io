---
layout: post

title: Thesis review, Minimum Covariance Determinant

tags: [Linear Algebra, CSE, Data Science, AI, Statistics]

feature-img: "assets/img/0.post/linear/2.png"

thumbnail: "assets/img/0.post/linear/2.png"

categories: CSE, AI
---

## THESIS REVIEW: Minimum Covariance Determinant

이 글은 [**Hubert, M., & Debruyne, M. (2010). Minimum covariance determinant.**](https://wis.kuleuven.be/stat/robust/papers/2010/wire-mcd.pdf)

논문을 토대로 작성되었습니다. <br>

### 1. ABSTRACT

**MCD(Minimum Covariance Determinant)방법**은 빠른 알고리즘을 통해 **다변량 위치(location)** 와 **산포(scatter)** 의 **강건한(robust) 추정치**를 제공하는 기법이다. <br>

**공분산 행렬(Covariance matrix)** 는 다양한 통계방법의 초석이 되므로, MCD는 강건하고 계산적으로 효율적인 다변량 기법들을 개발하는데도 사용되고 있다. <br>

더 나아가 MCD는 **이상치(outlier) 감지를 위한 효율적이고 실용적인 도구**로 활용될 수 있어, 데이터 분석의 신뢰성을 높이는 데 기여한다. <br>

<br>

**MCD estimator**는 **affine equivariance**, **breakdown value**, 그리고 **influence function**의 주요 속성에 대한 내용과 <br>

affine equivariant이면서 robust를 상속하는 **빠른 결정론적 알고리즘**과 차원의 수가 case보다 많은 수 있는 고차원에 대해 설계되고 특이 행렬을 방지하기 위한 **정규화**에 대한 최근 확장 개념이 묘사되어있다. <br>

MCD는 의학, 금융, cv등 다양한 분야에 적용되고, **PCA(주성분분석)**, **Regression(회귀분석)**, **Factor analysis(요인분석)** 등 다변량 기술을 개발하는데도 사용되고 있다.

우선 MCD를 사용해야 하는 이유에 대한 예시를 살펴보자.

### 2. DESCRIPTION OF THE MCD ESTIMATOR

다변량 위치와 산포의 세팅에서, 데이터는 nxp 데이터 행렬에 저장되어있다고 가정한다.

$$
X=(x1​,…,xn​)⊤
$$

여기서 각 xi는 i번째 **관측값(i-th observation**이다.<br>

따라서 **n은 객체(object)의 개수**, **p는 변수(variable)의 개수**를 의미한다.

<br>

**Figure 1, Bivariate wine data with classical and robust tolerance ellipse**

<img width="738" height="584" alt="image" src="https://github.com/user-attachments/assets/26306fc1-00c0-45fb-bc23-43da31506fb7" />


<br>

예시를 위해 **이변량(bivariate, p=2) 데이터 집합**을 고려한다.<br>

이 논문에서는 와인 데이터셋을 고려하는데 이 데이터셋은 3종류의 이탈리아 와인에서 발견된 13가지 성분의 양을 포함되어 잇고, 첫번째 그룹에 속한 59개의 와인중 **사과산(malic acid)**, **프롤린(proline)** 성분에 집중하고 있다. <br>

위 **Figure 1**에 그 데이터의 산점도(scatter plot)이 제시되어있고, **classical** 및 **rubust** 97.5% **허용타원(tolerance ellipse)** 가 함께 그려져있다.

<br>

#### 2.1 Mahalanobis distance (마할라노비스 거리)

여기서 잠시 mahalanobis distance의 개념에 대해 짚고 넘어가자. <br>

공식은 다음과 같다.

$$
MD(x) = \sqrt{(x - \bar{x})^{\top} S^{-1} (x - \bar{x})}
$$

우선 공식을 이해하기 위해서는 **이차형식(quadratic form)** 에 대한 개념을 알고 있어야 한다. <br> 공식 유도과정은 다음과 같다.

$$
\textbf{가정: }\;\Sigma \in \mathbb{R}^{p \times p}\text{ 는 대칭 양의정부호(SPD) 공분산 행렬, }\;\mu \in \mathbb{R}^p
$$

$$
\Sigma = U \Lambda U^\top,\quad
\Lambda = \mathrm{diag}(\lambda_1,\dots,\lambda_p),\;\lambda_i>0
$$

$$
\Sigma^{-1/2} = U \Lambda^{-1/2} U^\top,\quad
\Lambda^{-1/2}=\mathrm{diag}(\lambda_1^{-1/2},\dots,\lambda_p^{-1/2})
$$

$$
z := \Sigma^{-1/2}(x-\mu)
$$

$$
\|z\|_2^2
= (x-\mu)^\top \big(\Sigma^{-1/2}\big)^\top \Sigma^{-1/2} (x-\mu)
$$

$$
= (x-\mu)^\top \Sigma^{-1} (x-\mu)
$$

$$
\therefore\; MD(x) = \sqrt{(x-\mu)^\top \Sigma^{-1} (x-\mu)}
$$

쉽게 설명하자면 거리는 절대값이기때문에 루트에 제곱을 씌워 다음과같은 형태임을 알수있다.

$$
\sqrt{(x - \mu)^2}
$$

이는 내적의 정의에 따라 다음과 같다

$$
\sqrt{(x-\mu)^\top (x-\mu)}
$$

여기서 루트안에 잇는 다음의 식을 **이차형식**으로 생각하면 반지름이 1인 원이 나온다<br>

하지만 거리를 계산할때 **분산(variance)** 의 존재때문에 (퍼짐의정도) 거리가 객관적으로 나오지 않고 밀도가 높은 곳은 거리가 덜반영되고, 낮은 곳은 더 크게 반영되는 문제가 있다. <br>

따라서 공분산으로 나눠 밀도를 맞춰주고 이를 **whitening**이라고 한다. <br>

그리고 이 과정을 통해 **고유치(eigen value)** 를 계수로 삼는 **타원(ellipse)** 이 나온다. <br>

이 과정을 생각하면 다음의 공식으로 바로 유도할 수 있다

$$
\therefore\; MD(x) = \sqrt{(x-\mu)^\top \Sigma^{-1} (x-\mu)}
$$

<br>

이 마할라노비스거리는 점 x가 data cloud의 중심으로 부터, 그 크기에 비해 **얼마나 멀리 떨어져 있는지**를 알려준다.

**참고** <br>

확률분포에서 어느 방향으로의 퍼진정도가 **분산(variance)** 이고 이 분산은 다시 말해 **고유치(eigen value)** 가 된다.<br>

그리고 그 방향은 **고유벡터(eigen vector)** 가 된다.

<br>

#### 2.2 Robust Distance

그럼 본론으로 돌아와서 **classical**과 **robust** 의 **tolerance ellipse**를 비교해보자. <br>

**Classical tolerance ellipse**는 MD(x)가 **카이제곱분포**의 분위수가 같아지는 p-차원 점 x들의 집합으로 정의한다.

밑 **Figure 2**는 **Mahalanobis distance**와 **robust distance**를 비교한 그림이다.

<br>

**Figure 2**

<img width="1570" height="608" alt="image" src="https://github.com/user-attachments/assets/81ac5339-3ad6-4ebc-9721-fedee473616c" />


<br>

Figure 2에서 보이다 싶이 기본 마할라노비스 거리의 타원은 모든 관측치를 포괄하려는 것을 알 수 있다. 따라서 이상치가 3개밖에 안보이는 것을 알 수 있다.<br>

반면에 Figure1의 **robust tolerance ellipse**는 다음식을 따르고 더 작고 정규화된 데이터 부분만 포함하고 있다.

$$
RD(x) = \sqrt{(x - \hat{\mu}_{\text{MCD}})^\top \hat{\Sigma}_{\text{MCD}}^{-1} (x - \hat{\mu}_{\text{MCD}})}
$$

여기서 μ_MCD는 위치의 mcd추정량이고, Σ_MCD는 mcd의 공분산 추정치 이다. <br>

이롤 이용하면 Figure 2(b)처럼 8개의 이상치와 1개의 이상치를 찾아내는 것을 볼 수 있다. <br>

**classical estimates**가 **이상치(outlying values)** 의 영향을 너무 크게 받아, MD(x)와 같은 확인 수단들이 더이상 이상치를 탐지할 수 없게 되는 현상을 **masking effect**라고 한다. <br>

이런 데이터의 신뢰할 수 있는 분석을 위해 **robust estimator**가 필요하다.

<br>

### 3. Definition

우선 [.]은 **바닥함수(floor function)** 을 나타낸다. <br>

x를 넘지않는 최대의 정수를 뜻하는 것으로 흔히 아는 **Gauss symbol**이다. <br>

매개변수 **[(n + p + 1)/2] ≤ h ≤ n** 갖는 **raw MCD 추정량**은 다음과 같은 **위치(location)** 와 **산포(dispersion)** 추정치를 정의한다. <br>

> μ_0은 표본 공분산 행렬의 행렬식이 최소가 되는 h개의 관측치들의 평균이다 <br>
> 
> Σ는 공분산행렬에 consistency factor(보정계수) **c**를 곱한 것이다.

<br>

여기서 MCD 추정량은 **h>p**일때만 계산이 가능한데, 그렇지 않으면 어떤 h의 subset의 공분산행렬이 **특이(singular)** 하게 된다. <br>

이유에 대해 잠시 알아보고 넘어가자

<br>

<br>

#### 3-1 Invertibility Condition

**MCD의 목적은 이상치에 둔감한(robust) 공분산 추정치를 만드는 것이다**. <br>

때문에 n개의 데이터중 **h개의 관측치(이상치가 덜 포함된 subset)** 을 찾아서 평균, 공분산을 계산한다. <br>

여기서 h개의 subset을 추출했을때 나오는 데이터 행렬은 **hxp** 행렬일 것이다.<br>

$$
X =
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1p} \\
x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{h1} & x_{h2} & \cdots & x_{hp}
\end{bmatrix}
\in \mathbb{R}^{h \times p}
$$

여기서 공분산 행렬을 구하면 다음과 같다.

$$
\hat{\Sigma}_{MCD} = \frac{1}{h} \sum_{i=1}^{h} (x_i - \bar{x}_h)(x_i - \bar{x}_h)^\top
$$

공식의 의미를 부가설명하자면 이상치가 덜 포함된 subset을 추출했기때문에 원래의 평균으로 빼버리면 이상치로 왜곡된 평균을 빼버리는것이기 때문에 MCD의 강건성이 떨어진다. <br>

따라서 subset의 평균을 구하고 그걸로 다시 빼서 평균을 구한다. 그리고 이걸 **중심화**라고 한다.<br>

이때 중심화로 인해 열의 합이 0이 되서 **종속**이 된다. 따라서 정확한 조건은<br>

**h-1 > p** 가 더 정확하다. 그리고 그 종속이 되는 한 행은 **표본평균**이다.<br>

<br>

조건에 대해 설명하기 전에 다음을 먼저 기억하자. <br>

> rank(Σ) < dim => 비가역(invertible) <br>
> 
> 따라서 rank(Σ) = p여야 가역이다. (pxp 행렬)

여기서 h-1>p인 이유는 공분산 행렬에서 (x - x바)를 Z_h라고 할때 <br>

$$
Z_h \;=\; X - \mathbf{1}_h\,\bar{x}_h^{\top}
\qquad(\bar{x}_h=\tfrac{1}{h}\sum_{i=1}^h x_i)
$$

$$
\text{(일반위치 가정 하에)}\quad
1 \;\le\; \operatorname{rank}(Z_h) \;\le\; \min\!\big(p,\;h-1\big)
$$

원래의 조건이면 0 <= rank(A) 겠지만 Z는 영행렬이 아니므로 1보다 커야한다. <br>

여기서 만약 h-1 < p라고 가정해보자. <br>

그럼 최대 rank가 h-1이기 때문에 공분산행렬 계산결과 또한 h-1 x h-1 행렬이 나올것이다. <br>

근데 여기서 h-1 < p 이므로 rank(Σ) <= h-1 < p = dim이 되어 **비가역**이 된다. <br>

따라서 **h-1 > p** 라는 조건이 필요하다.

<br>
<br>

#### 3-2 elliptically symmetric unimodal distributions

MCD 추정량은 타원대칭(elliptically symmetric)이며, unimodal한 분포를 위해 설계되었다. <br>

매개변수가 μ ∈ R^p 이고, Σ가 크기 p의 **양의 정부호 행렬**인 다변량 분포가 다음 조건을 만족하면 타원 대칭이고 **단봉(unimodal)** 이라 한다

어떤 **엄격히 감소하는 함수 g** 가 존재하여, 밀도가 다음과 같은 형태로 쓸 수 있을 때이다.

$$
f(x) \;=\; \frac{1}{\sqrt{|\Sigma|}} \; g\!\left((x - \mu)^\top \Sigma^{-1} (x - \mu)\right)
$$

우선 **양정치행렬**을 가져야 하는이유는 **이차형식**이 **양수**여야 거리처럼 쓸 수 있기 때문이다.<br>

그리고 앞에 det(Σ)를 제곱근으로 나눠주는이유는 확률밀도함수의 적분 총합은 1이여야 하기 때문이다. <br>

여기서 선형변환의 넓이에 대한 이해가 있어야 한다.<br>

간단히 설명하자면, 선형변환 T에 대한 S가 T의 정의역에 해당하는 부분집합이면 T(S)의 넓이는 다음과 같다.

$$
T(S)의 \ 넓이 \ = \ |detA| \times S의 넓이
$$

**Jacobian matrix**를 알고있다면 이해하기 쉽다. <br>

우선 단위구로 하면 편하겠지만 분산이 1이 아닌 방향마다 다른 고유치가 있기때문에 다음 선형변환을 통해 타원으로 맞춰준다. <br>

$$
y \;=\; \Sigma^{-\tfrac{1}{2}} (x - \mu)
$$

위 선형변환을 통해 넓이는 det(Σ)제곱근에 비례하게 되고 이를 1로 맞추기 위해 나눠준다.

<br>
<br>

#### 3-3 Reweighted Estimator

MCD 추정에서 h의 의미는 전체표본 n개 중에서 이상치 없는 h개의 관측치를 골라서 subset를 형성해서 **location**과 **scatter**를 계산하는 것이다. <br>

여기서 h값이 작을수록 많은 이상치(outlier)를 버릴수 있어 높은 **breakdown point**를 얻지만 많이 버린다는 것은 활용량이 줄어든다는 것이다. <br>

<br>

추후 언급하겠지만 **breakdown point**는 **데이터 중 일부를 이상치로 두었을때 추정량이 망가지는 최소 비율**이다.<br>

예를들어 **평균**은 한개의 이상치만 추가되도 확 변할 수 있기 때문에 breakdown value가 낮다고 볼 수 있다. <br>

하지만 **중앙값(median)** 은 절반 이상을 이상치로 바꾸는 거 아니면 변하지 않는다. 따라서 breakdown value는 50이다.

<br>

MCD추정량은 h = [n+p+1]/2 에서 가장 robust하다. 이는 모집단 수준에서 대략 절반에 해당한다. <br>

하지만 이 경우 효율성이 낮다는 단점이 있다. <br>

예를 들어 변수가 2개 일때 점근상대효율은 공분산행렬에 비해 6퍼 밖에 안되고 10개일 때는 20.5밖에 되지 않는다. <br>

때문에 효율성을 위해 더 큰 값을 고려할 수도 있겠지만 이러면 **견고성**을 감소시킨다.

<br>

때문에 효율성을 높이면서 높은 견고성을 위해 **재가중치 추정량(reweighted estimator)** 을 사용한다.

$$
\hat{\mu}_{MCD} \;=\; \frac{\sum_{i=1}^n W(d_i^2) x_i}{\sum_{i=1}^n W(d_i^2)}
$$

$$
d_i \;=\; \sqrt{(x - \hat{\mu}_0)^t \,\hat{\Sigma}_0^{-1}\,(x - \hat{\mu}_0)}

$$

$$
\hat{\Sigma}_{MCD} \;=\; c_1 \frac{1}{n} \sum_{i=1}^n W(d_i^2)\,(x_i - \hat{\mu}_{MCD})(x_i - \hat{\mu}_{MCD})^t
$$

수식에 대해 설명하자면

> W(.) : 가중함수, d: 마할라노비스 거리<br>
> 
> 분모: 가중치 총합, 분자: 가중치의 합
> 
> c : 일치성 계수(consistency factor)

여기서 마할라노비스에 있는 공분산은 **초기 MCD**이고 재가중치 mcd가 아니다.<br>

분자에서 점에 대한 이상치일수록 W가 0이 되어 반영안되고 정상치일수록 1에 가까워저 많이 반영된다. <br>

W에 대해 간단하면서 효과적인 선텍은 다음과같다

$$
W(d^2) = I \big( d^2 \leq \chi^2_{p,0.975} \big)
$$

그리고 이는 p=2일때 최대 45.5%, p=10에서는 82%까지 증가시킨다.<br>

<br>

### 4. Outlier detection

Robust MCD 추정량은 이상치를 탐지하는데 매우 유용하지만, Robust distacne는
