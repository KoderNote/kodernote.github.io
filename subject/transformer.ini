https://medium.com/@hugmanskj/transformer%EC%9D%98-%ED%81%B0-%EA%B7%B8%EB%A6%BC-%EC%9D%B4%ED%95%B4-%EA%B8%B0%EC%88%A0%EC%A0%81-%EB%B3%B5%EC%9E%A1%ED%95%A8-%EC%97%86%EC%9D%B4-%ED%95%B5%EC%8B%AC-%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4-%ED%8C%8C%EC%95%85%ED%95%98%EA%B8%B0-5e182a40459d

Transformer는 ai에서 다루어지는 중요한 네트워크 아키텍처임

transforer기술은 연계되어있는 표현학습과 seq2seq연결점을 파악해야함

표현학습
ai를 학습시키기위해 세상을 기계가 이해할수있는 형태로 표현해야함
데이터를 숫자, 벡터, 행렬등의 수학적 객체로 변환하여 기계가 이해할 수 있도록
만드는 것을 의미함

여기서 핵심은 복수의 아이템들(단어, 픽셀, 음성신호등)을 하나의 유의미한 벡터로 만드는것임
이 벡터는 해당 시퀀스의 의미를 담고있으며 ai는 이를 바탕으로 새로운 시퀀스를 생성함

seq2seq2
초기 이모델은 rnn을 사용해서 구현됨 rnn은 시퀀스 데이터를 처리하는데 효과적이였지만
몇가지 중대한 문제점에 직면함 그중하나가 기욱ㄹ기 소실문제와 기울기 폭발문제임

이는 attention메커니즘으로 해결됨
입력 시퀀스의 중요부분에 집중하여 필요한 부분만 선택적으로 추출함
이를통해 전체 시퀀스를 일괄적으로 처리하는것이 아닌 관련성이 높은 정보에
집중하여 효율적으로 처리함

rnn+attention
attention메커니즘으로 rnn기반 seq2seq모델은 크게 개선됨

transformer은 seq2seq구현의 패러다임을 완전히 바굼
transformer는 오로지 attention메커니즘으로 설계되어 seq2seq문제를 해결하는
새로운 방식을 제시함 
이모델은 복잡한 rnn구조를 제거하고 전체 시퀀스를 한번에 처리할 수 ㅣㅇㅆ는 구조로 설계됨
이로인해 학습 속도가 크게 향상되었고 더긴 시퀀스와 더깊은 네트워크에 대한 처리가 가능해짐

“Attention is All You Need”라는 논문에서 처음 소개됨
transformer를 통해 해결햐ㅏ고자 했던 주요문제는 다음과 같음
1.seq2seq를 위한 구조제공
2.다수항목 인코딩
3.장기의존성인코딩
4.순차적정보인코딩
5.빠른인코딩
6.단순한 아키텍처임
7.안정적인 학습

위의 질문에 대한 답
1. seq2seq를 위한 구조제공
transformer는 rnn+attention기반의 seq2seq구조를 대체하기 위해 만들어짐
특히 encoder+decoder구조를 그대로 구현함 이는 seq2seq구조와 동일함
2. 다수항목 인고딩 + 장기의존성 인코딩
우리가 살고있는 4차원공간(x y z time)에서 발생하는 대부분의 데이터는 시계열데이터임
이런 시계열은 여러개의 연속된 데이터포인트로 구성되고 이 복수의 데이터를 효과적으로 인코딩하여 하나의 의미있는 벡터로 표현하는 것이 매우 중요함


