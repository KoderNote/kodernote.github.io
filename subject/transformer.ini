https://medium.com/@hugmanskj/transformer%EC%9D%98-%ED%81%B0-%EA%B7%B8%EB%A6%BC-%EC%9D%B4%ED%95%B4-%EA%B8%B0%EC%88%A0%EC%A0%81-%EB%B3%B5%EC%9E%A1%ED%95%A8-%EC%97%86%EC%9D%B4-%ED%95%B5%EC%8B%AC-%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4-%ED%8C%8C%EC%95%85%ED%95%98%EA%B8%B0-5e182a40459d
https://wikidocs.net/31379

Transformer는 ai에서 다루어지는 중요한 네트워크 아키텍처임

transforer기술은 연계되어있는 표현학습과 seq2seq연결점을 파악해야함

표현학습
ai를 학습시키기위해 세상을 기계가 이해할수있는 형태로 표현해야함
데이터를 숫자, 벡터, 행렬등의 수학적 객체로 변환하여 기계가 이해할 수 있도록
만드는 것을 의미함

여기서 핵심은 복수의 아이템들(단어, 픽셀, 음성신호등)을 하나의 유의미한 벡터로 만드는것임
이 벡터는 해당 시퀀스의 의미를 담고있으며 ai는 이를 바탕으로 새로운 시퀀스를 생성함

seq2seq2
초기 이모델은 rnn을 사용해서 구현됨 rnn은 시퀀스 데이터를 처리하는데 효과적이였지만
몇가지 중대한 문제점에 직면함 그중하나가 기욱ㄹ기 소실문제와 기울기 폭발문제임

이는 attention메커니즘으로 해결됨
입력 시퀀스의 중요부분에 집중하여 필요한 부분만 선택적으로 추출함
이를통해 전체 시퀀스를 일괄적으로 처리하는것이 아닌 관련성이 높은 정보에
집중하여 효율적으로 처리함

rnn+attention
attention메커니즘으로 rnn기반 seq2seq모델은 크게 개선됨

transformer은 seq2seq구현의 패러다임을 완전히 바굼
transformer는 오로지 attention메커니즘으로 설계되어 seq2seq문제를 해결하는
새로운 방식을 제시함 
이모델은 복잡한 rnn구조를 제거하고 전체 시퀀스를 한번에 처리할 수 ㅣㅇㅆ는 구조로 설계됨
이로인해 학습 속도가 크게 향상되었고 더긴 시퀀스와 더깊은 네트워크에 대한 처리가 가능해짐

“Attention is All You Need”라는 논문에서 처음 소개됨
transformer를 통해 해결햐ㅏ고자 했던 주요문제는 다음과 같음
1.seq2seq를 위한 구조제공
2.다수항목 인코딩
3.장기의존성인코딩
4.순차적정보인코딩
5.빠른인코딩
6.단순한 아키텍처임
7.안정적인 학습

위의 질문에 대한 답
1. seq2seq를 위한 구조제공
transformer는 rnn+attention기반의 seq2seq구조를 대체하기 위해 만들어짐
특히 encoder+decoder구조를 그대로 구현함 이는 seq2seq구조와 동일함
2. 다수항목 인고딩 + 장기의존성 인코딩
우리가 살고있는 4차원공간(x y z time)에서 발생하는 대부분의 데이터는 시계열데이터임
이런 시계열은 여러개의 연속된 데이터포인트로 구성되고 이 복수의 데이터를 효과적으로 인코딩하여 하나의 의미있는 벡터로 표현하는 것이 매우 중요함
이를 해결하기위해 attention메커니즘이 transformer이전에 개발됨
attention은 특정 데이터 포인트가 다른 데이터포인트와 얼마나 관련되어있는지 파악하고
이를 통해 입력데이터의 전체적인 맥락을 파악할 수 있게 해줌
이는 rnn과 같은 이전모델들이 각 시점의 데이터를 순차적으로 처리하는 것과 대비됨
특히 ,attention메커니즘은 전체 데이터포인트를 고려해서 각 데이터포인트의 중요도를 파악하고 
이를 기반으로 요약벡터를 생성함
transformer는 이 attention메커니즘을 기반으로 복수의 데이터를 효과적으로 인코딩하고 장기의존성을 처리하는것
몇가지 접근방식을 제안함

2-1. 멀티헤드 어텐션(multi-ghead attention)
transformer는 단일 어텐션 메커니즘 대신 멀티헤트 어텐션을 사용함
이는 동일한 데이터에 여러개의 어텐션 메커니즘을 병렬로 적용해서 다양한 방식으로 정보를 수집하고 분석할 수 있게함
이를 통해 모델은 다양한 측면을 동시에 고려할수 있고 결과적으로 더 풍부하고 다차원의 데이터를 얻을수 있음

2-2. 셀프 어텐션(self attention)
이는 입력 데이터 내의 각 요소가 서로 어떻게 상호작용하는지 모델링하는 바잇ㄱ임
이 메커니즘은 입력 시퀀스 내의 각요소(단어나 픽셀)를 query로 간주하고 생당 query와 입력 시퀀스내의 모든요소들key와의
관계를 평가함 이과정에서 key에 대한 가중치가 계산되고 이를 통해 query에 가장 관련이 높은 정보가 강조됨
겨로가적으로 각 요소는 다른 모든 요소와의 관계를 반영해서 자신의 표현을 조정하게 됨
이런 과적ㅇ은 장기의존성문제를 해결하는데 유용하고 시퀀스 내에서 멀리 떨어진 요소간의 상호작용도 효과적으로 포착함
query key value라는 개념을 사용해서 내부적인 상호작용을 모델링해서 selfattention이라는 명칭이 붙음

2-3.크로스 어텐션 
크로스 어텐션은 주로 트랜스포머 모델의 디코더 부분에서 활용되며, 인코더에서 생성된 표현과 디코더의 현재입력사이의 상호작용을]\
모델링하는데 사용됨. 이 메커니즘을 통해 디코더는 인코더가 제공하는 컨텍스트정보를 바탕으로 출력시퀀스의 다음 요소를 예측하며, 따라서 인코더의 정보를 효과적으로 활용할 수 있음
이런 과정은 특히 기계번여고가 같이 입력시퀀스를 다른현태의 출력 시퀀스로 변환하는 작업에 필수적임
크로스 어텐션은 인코더와 디코더사이의 교차도니ㅡㄴ 정보의 흐름을 나타내며 디코더의 query가 인코더의 key와 value와 상호작용하는 방식으로 구현됨
이런 서로 다른 컴포넌트간의 정보교환때문에 크로스 어텐션이라는 용어가 사용됨
이런 메커니즘은 다수의 데이터 포인트를 효과적으로 인코딩하고 장기의존성을 포함한 복잡한 시퀀스 구조를 이해하는데 중요한 역할을 함.


